> 集成学习的概念及主要方法
> * 包括 Bootstraping、Bagging、
> * 随机森林，随后计算随机森林中各个特征的重要性，找出对模型贡献较大的特征。

> 背景:
> 假设已经为某一特定问题选中了【最佳的模型】，想进一步【提升其准确率】，就需要应用一些更高级的机器学习技术：集成（Ensemble）
> Bootstrap 方法的流程如下:
> 假设有尺寸为 N 的样本 X，从该样本中【有放回】地随机抽取 N 个样本，以创建一个新样本。
> 换句话说，从尺寸为 N 的原样本中随机选择一个元素，并重复此过程 N 次。选中所有元素的可能性是一样的，因此每个元素被抽中的概率均为
> 1/N
> 假设用 Bootstrap 方法从一个袋子中抽球，每次抽一个。在每一步中，将选中的球放回袋子，这样下一次抽取是等概率的，
> 即从同样数量的 N 个球中抽取。注意，因为我们把球放回了，新样本中可能有重复的球。把这个新样本称为 X1.重复这一过程 M 次，创建
> M 个 Bootstrap 样本
> X1、X2、X3……Xm 。最后，我们的样本数量就从原先的 1 个扩充到了 M 个，就有充足的样本来计算原始分布的多种统计数据。


> 随机森林中的决策树基于原始数据集中不同的 Bootstrap 样本构建。对第 K 棵树而言，其特定 Bootstrap 样本大约留置了 37% 的输入。



> 如果使用随机森林或梯度提升遇到了严重的过拟合，可以试试极端随机树。
> 极端随机树（Extremely Randomized Trees）在节点分岔时应用了更多随机性。
> 和随机森林一样，极端随机树使用一个随机特征子空间。
> 然而，极端随机数并不搜寻最佳阈值，相反，它为每个可能的特征随机生成一个阈值，然后挑选最佳阈值对应的特征来分割节点，通过增加少量偏差来降低方差。





> 随机森林和k近邻的相似之处。随机森林的预测基于训练集中的【相似样本的标签】。这些样本越长出现在同一叶节点，它们的相似度就越高。
> 随机森林主要用于监督学习，不过也可以应用于无监督学习。
> 使用 scikit-learn 的 RandomTreesEmbedding() 方法将数据集转换为高维的稀疏表示，继而用于无监督学习中

## 随机森林的优劣势

> * 优势
>   1. 高预测准确率，在大多数问题上避险由于线性算法
>   2. 对于离散值的鲁棒性较好
>   3. 不需要精细的参数调整
>   4. 可处理连续值和离散值
>   5. 不容易出现过拟合。可以通过增加树的数量没来提升总体表现型，当树的数量增大到一定数值后，学习曲线趋于平稳。
>   6. 能够更好的处理数据缺失，即使有大部分的数据缺失现象，仍能保持较好的准确率
>   7. 决策树底层使用相似性计算可以用于【后续聚类、离散值检测或者兴趣数据表示】。
>   8. 支持无监督聚类，数据可视化和离散值检测
>   9. 易于并行化，伸缩性强。
>
> * 劣势
>   1. 随机森林的输出难解释
>   2. 特征重要性估计没有形式化的p值
>   3. 在数据稀疏是，表现不如线性模型好
>   4. 和线性回归不同，随机森林无法外推。
>   5. 在某些问题上容易过拟合，特别是处理高噪音数据时
>   6. 处理数据集不同的类别数据时，随机森林偏重数量级较高的数据，因为这更能明显的提高准确率
>   7. 所得模型较大，需要大容量的ram来支撑

## 特征的重要性
我们需要对算法的输出进行合理解释，在不能完全理解算法的情况下，至少希望能够找出那个输入特征对结果的贡献最大，
基于随机森林，我们可以相当容易地获取这类信息


> 评判标准：
> 在随机森林中，某一特征在所有树中离树根的平均距离越近，这一特征在给定的分类或回归问题中就越重要。

> **能否将一个弱模型通过某些手段得到一个强大的模型**
> 答案是肯定的，集成方法就是其中一类，集成学习的算法有很多，先从最简单的开始AdaBoost
> 在算法学习时会给数据样本的每个都会赋【一个权重值】，每一个基分类器【开始分类之前】都会根据【前一个基分类器】的分类来调节权重值
> 通过线性组合的方式将基分类器集成起来得到一个强模型。
> 当进行迭代时，可以看到误分类点权重的增加，特别是在类之间的边界上。当数据有怪异的异常值时，过拟合现象较为严重。
> 为了解决AdaBoost的过拟合问题，1999年提出了泛化版本machine也成为【GBM】
> 回归问题我们选择的损失函数为军方误差