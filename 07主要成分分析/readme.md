> 无监督学习是机器学习中的一类重要算法。与监督学习算法相比，无监督学习算法不断输入数据进行标记，即不需要给出标签或者类别
> 此外无监督学习还可以在没有辅助的情况下能够学习数据的内在关系
> 由于数据不需要手动标记标签，这可以是许多无监督学习算法能够处理大量的数据，从而节省大量的人力成本。因为无监督学习算法是没有使用标签数据。
> 所以一般情况下很难直观地评估无监督学习算法的质量。


> 在无监督学习中最常见的任务之一是降维，也就是减少输入数据的维数。 
> 为什么要降维呢？主要有一下几个原因：
>  1. 首先，降维可能有助于数据可视化，因为人是无法理解高维数据的，通常只能看明白二维或三维数据的可视化图像。
>  2. 其次，降维可以有效地解决维度灾难的问题，改善模型的训练效果。
>  3. 此外，降维还可以进行数据压缩，提升模型训练效率。

> ![主成分分析](C:\Users\Administrator\Desktop\ai人工智能赛道\07主要成分分析\主成分分析.jpeg)
>
> 如果将坐标系中的数据点投影到X轴或者y轴哪一个更容易区分。
> 很显然是x轴因为数据投影到x轴之后相比于y轴更加分散，也就是数据样本的方差更大，从而更容易区分。
> 如果要在坐标系中找到一根轴，使的投影到这根轴后的数据方差更大，也就是说更分散。显然就是图中缩表的椭圆轴的方向
> PCA的思想就是这样。如果将图中的数据投影到椭圆轴上则数据也就从二维将为一维。
> 想要保留一个矩阵的最大信息，我们只需要保留该矩阵的最大特征值所对应的特征向量所组成的矩阵即可


