[TOC]

## 最近邻方法是另一种非常流行的分类方法。当然，也可也用于回归问题。
> 如果样本间的距离能以足够好的方法衡量，那么相似的样本更可能属于同一分类。
> 举例：
> 如果想知道蓝牙耳机属于什么类别，你可以查找五个相似的耳机，如果其中4个标记为【配件】类，只有一个标记为【科技】类
> 那么根据最近邻方法属于【配件】类

> 在最近邻方法中，为了对测试集中的每个样本进行分类，需要依次进行以下操作：
> 1. 计算训练集中每个样本之间的距离
> 2. 从训练集中选取k个距离最近的样本
> 3. 测试样本的类别将是它k个最近邻中常见的分类

## 验证模型的好坏
留置法 : 保留一部分数据作为留置集其余训练 保留20-40
交叉验证 : 在 k 折交叉验证中，模型在原数据集的 K−1 个子集上进行训练，然后在剩下的 1 个子集上验证表现，
         重复训练和验证的过程，每次使用不同的子集，总共进行 K 次，由此得到 K 个模型质量评估指数，
         通常用这些评估指数的求和平均数来衡量分类/回归模型的总体质量。


> 决策树的正确率:0.94 max_depth = 5
> k近邻的正确率:0.881 k=10
> 模型调优，交叉验证对每次的**最大深度**和**最大特征数**进行调优
> GridSearchCV() 函数简单的实现交叉验证。把全部数据分成五份计算模型的表现。
> 


> 在这个任务里，决策树有着 94%/94.6%（留置法/交叉验证调优后）的准确率，
> k-NN 有着 88%/89%（留置法/交叉验证调优后）的准确率，显然决策树的表现更好。
> 

## 决策树和最近邻的优劣势
> 决策树
> * 优势
>   1. 生成容易理解的分类规则，这一属性称为模型的可解释性。 【例如 生成容易理解的分类规则，这一属性称为模型的可解释性。】
>   2. 很容易可视化，即**模型本身**和**特定测试对象**(穿过树的路径)的预测可以【被解释】
>   3. 训练和预测的速度快
>   4. 较少的参数数目
>   5. 支持数值和类别的特征 
> * 劣势
>   1. 对输入数据中的【噪音特别敏感】
>   2. 决策树构建的边界有其局限性：它由垂直于其中一个坐标轴的超平面组成，在实践中比其他方法的效果要差。
>   3. 需要通过剪枝，设置叶结点最小样本数，设置树的最大深度等方法避免过拟合
>   4. 不稳定性，数据的细微变化都会显著的改变决策树。
>   5. 倘若数据中有缺失值，将难以构建决策树模型
>   6. 只能内插不能外推，假如你预测的对象在训练集所设置的特征空间之外

> 最近邻
> * 优势
>   1. 实验简单
>   2. 研究充分
>   3. 通常而言，在分类、回归、推荐问题中第一个值得尝试的方法
>   4. 通过选择恰当的衡量标准或核，它可以适应某一特定问题
> * 劣势
>   1. 速度较快，但是在现实生活中，用于分类的邻居数目通常较大(100-150)在这一情形下，knn不如决策树
>   2. 如果数据集有很多变量，很难找到合适的权重，也很难判定哪些特征对分类/回归不重要
>   3. 依赖与对象之间的距离度量，默认选项欧几里得距离常常是不合理了的选项，可以通过网格搜索参数得到良好的解，但在大型的数据集上耗时很长
>   4. 没有理论来指导我们k的选择，进而只能进行网格搜搜。【在邻居数较小的情形下，该方法对于离散值很敏感。有过拟合的倾向。】[选择决策树]
>   5. 当数据集存在很多特征时它的表现不佳

